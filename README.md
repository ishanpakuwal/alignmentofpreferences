# Reproducing Preference Alignment on Anthropic HH with a Small GPT

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**CS329H: Machine Learning from Human Preferences - Final Project**  
*Authors:* Ishan Pakuwal and Tanaya Yadav  
*Institution:* Stanford University  
*Date:* December 2025

## ðŸ“– Overview

This repository contains a complete, reproducible implementation of preference alignment for language models using:
- **Supervised Fine-Tuning (SFT)** on preferred responses from the Anthropic HH dataset
- **Direct Preference Optimization (DPO)** for aligning with human preferences
- **Comprehensive evaluation** across 5 metrics: perplexity, toxicity, refusal accuracy, human preferences, and response quality

We train an 85M-parameter GPT-style model on a single GPU in under 3 hours, achieving:
- **68% win rate** in pairwise human preferences vs. SFT baseline
- **45% reduction** in toxicity (mean score: 0.089 â†’ 0.049)
- **18-point improvement** in refusal accuracy (74% â†’ 92%)

## ðŸŽ¯ Key Features

- âœ… **Single-GPU Training**: Runs on free Google Colab T4 GPU
- âœ… **Fast Iteration**: Complete pipeline in 2-3 hours
- âœ… **Minimal Dependencies**: Based on nanoGPT for clarity
- âœ… **Comprehensive Evaluation**: 5 complementary metrics
- âœ… **Fully Reproducible**: Fixed seeds, exact package versions
- âœ… **Educational**: Designed for learning preference alignment

## ðŸ“‚ Repository Structure

```
cs329h-preference-alignment/
â”œâ”€â”€ CS329H_Complete_Project.ipynb    # Main notebook with complete pipeline
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ requirements.txt                  # Python dependencies with versions
â”œâ”€â”€ verify_setup.py                   # Environment verification script
â”œâ”€â”€ EVALUATION_METHODOLOGY.md         # Detailed evaluation protocols
â”œâ”€â”€ results/                          # Generated outputs (created at runtime)
â”‚   â”œâ”€â”€ sft_final_model.pt           # Supervised fine-tuned model checkpoint
â”‚   â”œâ”€â”€ dpo_final_model.pt           # DPO-optimized model checkpoint
â”‚   â”œâ”€â”€ best_sft_model.pt            # Best SFT checkpoint (early stopping)
â”‚   â”œâ”€â”€ training_curves.png          # Training dynamics visualization
â”‚   â”œâ”€â”€ toxicity_distribution.png    # Toxicity scores comparison
â”‚   â”œâ”€â”€ model_comparison.csv         # Quantitative metrics table
â”‚   â””â”€â”€ results_summary.json         # Complete results in JSON format
â”œâ”€â”€ annotations/                      # Human evaluation data (created at runtime)
â”‚   â”œâ”€â”€ pairwise_preferences_for_annotation.csv
â”‚   â””â”€â”€ helpfulness_for_annotation.csv
â””â”€â”€ paper/                            # Final manuscript
    â”œâ”€â”€ manuscript.tex
    â”œâ”€â”€ refs.bib
    â””â”€â”€ manuscript.pdf
